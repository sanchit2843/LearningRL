{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/Reinforcementlearningprojects/blob/master/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4MdlxIEAsvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "368215a9-a6c8-480f-d0e1-d5d833b1a0f1"
      },
      "source": [
        "!sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev ffmpeg xvfb xorg-dev python-opengl libboost-all-dev libsdl2-dev swig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 69%\r\rReading package lists... 69%\r\rReading package lists... 69%\r\rReading package lists... 70%\r\rReading package lists... 70%\r\rReading package lists... 75%\r\rReading package lists... 75%\r\rReading package lists... 75%\r\rReading package lists... 75%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "xorg-dev is already the newest version (1:7.7+19ubuntu7.1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.3).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MojpEOJHEWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4dc9fdfb-cf78-4499-ff92-5742c5e2102b"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oye8_PrHKKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "572874d9-fd93-4bb4-c51b-46a853548d09"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[len(mp4list)-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 10:54:59.813594 139926530979712 abstractdisplay.py:144] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRFponvkD6iQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3909
        },
        "outputId": "70e85b8f-fa56-4ca8-d61a-94d95c3417b5"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "\n",
        "#hyper parameters\n",
        "\n",
        "GAMMA = 0.99\n",
        "\n",
        "LR = 0.001\n",
        "MEMORY_CAPACITY = 10000\n",
        "Q_NETWORK_ITERATION = 300\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "EPISODES = 500\n",
        "env = wrap_env(gym.make('MountainCar-v0'))\n",
        "NUM_STATES = env.observation_space.shape[0] # 2\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(NUM_STATES, 100)\n",
        "        self.fc1.weight.data.normal_(0, 0.1)\n",
        "        self.fc2 = nn.Linear(100, NUM_ACTIONS)\n",
        "        self.fc2.weight.data.normal_(0, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Dqn():\n",
        "    def __init__(self):\n",
        "        self.eval_net, self.target_net = Net().to('cuda'), Net().to('cuda')\n",
        "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES *2 +2))\n",
        "        # state, action ,reward and next state\n",
        "        self.memory_counter = 0\n",
        "        self.learn_counter = 0\n",
        "        self.optimizer = optim.SGD(self.eval_net.parameters(), LR,momentum = 0.9)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "\n",
        "    def store_trans(self, state, action, reward, next_state):\n",
        "        index = self.memory_counter % MEMORY_CAPACITY\n",
        "        trans = np.hstack((state, [action], [reward], next_state))\n",
        "        self.memory[index,] = trans\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # notation that the function return the action's index nor the real action\n",
        "        # EPSILON\n",
        "        state = torch.unsqueeze(torch.cuda.FloatTensor(state) ,0)\n",
        "        \n",
        "        if np.random.randn() <= EPSILON:\n",
        "            action_value = self.eval_net.forward(state)\n",
        "            action = torch.max(action_value, 1)[1].data.cpu().numpy() # get action whose q is max\n",
        "            action = action[0] #get the action index\n",
        "        else:\n",
        "            action = np.random.randint(0,NUM_ACTIONS)\n",
        "        return action\n",
        "\n",
        "    def plot(self, ax, x):\n",
        "        ax.cla()\n",
        "        ax.set_xlabel(\"episode\")\n",
        "        ax.set_ylabel(\"total reward\")\n",
        "        ax.plot(x, 'b-')\n",
        "        plt.pause(0.000000000000001)\n",
        "\n",
        "    def learn(self):\n",
        "        # learn 100 times then the target network update\n",
        "        if self.learn_counter % Q_NETWORK_ITERATION ==0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.learn_counter+=1\n",
        "\n",
        "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
        "        batch_memory = self.memory[sample_index, :]\n",
        "        batch_state = torch.cuda.FloatTensor(batch_memory[:, :NUM_STATES])\n",
        "        #note that the action must be a int\n",
        "        batch_action = torch.cuda.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n",
        "        batch_reward = torch.cuda.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n",
        "        batch_next_state = torch.cuda.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
        "\n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        q_next = self.target_net(batch_next_state).detach()\n",
        "        q_target = batch_reward + GAMMA*q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
        "\n",
        "        loss = self.loss(q_eval, q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        #self.scheduler.step()\n",
        "\n",
        "\n",
        "def main():\n",
        "    net = Dqn()\n",
        "    print(\"The DQN is collecting experience...\")\n",
        "    step_counter_list = []\n",
        "    max_position = -0.4\n",
        "    EPSILON = 0.65\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        step_counter = 0\n",
        "        if(episode%100==0):\n",
        "          EPSILON = EPSILON+0.05\n",
        "        while True:\n",
        "            step_counter +=1\n",
        "            env.render()\n",
        "            action = net.choose_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            reward = next_state[0] + 0.5\n",
        "            \n",
        "            if next_state[0] > max_position:\n",
        "              max_position = next_state[0]\n",
        "            net.store_trans(state, action, reward, next_state)\n",
        "            if net.memory_counter >= MEMORY_CAPACITY:\n",
        "                net.learn()\n",
        "                if done:\n",
        "                    print(\"episode {}, the reward is {}\".format(episode, round(reward, 3)))\n",
        "            if done:\n",
        "                step_counter_list.append(step_counter)\n",
        "                net.plot(net.ax, step_counter_list)\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "    return net\n",
        "agent = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The DQN is collecting experience...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "episode 49, the reward is 0.003\n",
            "episode 50, the reward is 0.071\n",
            "episode 51, the reward is 0.15\n",
            "episode 52, the reward is 0.15\n",
            "episode 53, the reward is 0.115\n",
            "episode 54, the reward is 0.4\n",
            "episode 55, the reward is 0.115\n",
            "episode 56, the reward is 0.168\n",
            "episode 57, the reward is 0.224\n",
            "episode 58, the reward is 0.154\n",
            "episode 59, the reward is -0.01\n",
            "episode 60, the reward is 0.127\n",
            "episode 61, the reward is 0.203\n",
            "episode 62, the reward is 0.136\n",
            "episode 63, the reward is 0.095\n",
            "episode 64, the reward is 0.21\n",
            "episode 65, the reward is 0.158\n",
            "episode 66, the reward is 0.172\n",
            "episode 67, the reward is 0.083\n",
            "episode 68, the reward is 0.104\n",
            "episode 69, the reward is 0.058\n",
            "episode 70, the reward is 0.175\n",
            "episode 71, the reward is 0.058\n",
            "episode 72, the reward is -0.012\n",
            "episode 73, the reward is 0.194\n",
            "episode 74, the reward is 0.379\n",
            "episode 75, the reward is 0.168\n",
            "episode 76, the reward is 0.206\n",
            "episode 77, the reward is 0.08\n",
            "episode 78, the reward is 0.061\n",
            "episode 79, the reward is -0.037\n",
            "episode 80, the reward is 0.112\n",
            "episode 81, the reward is 0.082\n",
            "episode 82, the reward is 0.031\n",
            "episode 83, the reward is 0.118\n",
            "episode 84, the reward is 0.123\n",
            "episode 85, the reward is 0.135\n",
            "episode 86, the reward is 0.018\n",
            "episode 87, the reward is 0.099\n",
            "episode 88, the reward is -0.177\n",
            "episode 89, the reward is 0.134\n",
            "episode 90, the reward is 0.009\n",
            "episode 91, the reward is -0.007\n",
            "episode 92, the reward is 0.119\n",
            "episode 93, the reward is 0.026\n",
            "episode 94, the reward is -0.207\n",
            "episode 95, the reward is -0.001\n",
            "episode 96, the reward is -0.17\n",
            "episode 97, the reward is -0.28\n",
            "episode 98, the reward is 0.015\n",
            "episode 99, the reward is -0.038\n",
            "episode 100, the reward is 0.178\n",
            "episode 101, the reward is -0.089\n",
            "episode 102, the reward is 0.026\n",
            "episode 103, the reward is 0.171\n",
            "episode 104, the reward is 0.06\n",
            "episode 105, the reward is 0.002\n",
            "episode 106, the reward is 0.055\n",
            "episode 107, the reward is -0.267\n",
            "episode 108, the reward is 0.03\n",
            "episode 109, the reward is 0.108\n",
            "episode 110, the reward is -0.014\n",
            "episode 111, the reward is 0.078\n",
            "episode 112, the reward is 0.009\n",
            "episode 113, the reward is 0.046\n",
            "episode 114, the reward is -0.12\n",
            "episode 115, the reward is 0.153\n",
            "episode 116, the reward is 0.09\n",
            "episode 117, the reward is 0.106\n",
            "episode 118, the reward is 0.123\n",
            "episode 119, the reward is 0.133\n",
            "episode 120, the reward is 0.16\n",
            "episode 121, the reward is 0.065\n",
            "episode 122, the reward is 0.081\n",
            "episode 123, the reward is 0.131\n",
            "episode 124, the reward is 0.217\n",
            "episode 125, the reward is 0.238\n",
            "episode 126, the reward is 0.107\n",
            "episode 127, the reward is 0.197\n",
            "episode 128, the reward is 0.247\n",
            "episode 129, the reward is 0.015\n",
            "episode 130, the reward is 0.051\n",
            "episode 131, the reward is 0.112\n",
            "episode 132, the reward is 0.077\n",
            "episode 133, the reward is 0.13\n",
            "episode 134, the reward is -0.075\n",
            "episode 135, the reward is 0.042\n",
            "episode 136, the reward is -0.069\n",
            "episode 137, the reward is -0.164\n",
            "episode 138, the reward is 0.112\n",
            "episode 139, the reward is 0.079\n",
            "episode 140, the reward is 0.11\n",
            "episode 141, the reward is 0.039\n",
            "episode 142, the reward is -0.144\n",
            "episode 143, the reward is 0.184\n",
            "episode 144, the reward is 0.098\n",
            "episode 145, the reward is 0.041\n",
            "episode 146, the reward is -0.057\n",
            "episode 147, the reward is 0.145\n",
            "episode 148, the reward is 0.044\n",
            "episode 149, the reward is 0.108\n",
            "episode 150, the reward is 0.084\n",
            "episode 151, the reward is -0.179\n",
            "episode 152, the reward is -0.2\n",
            "episode 153, the reward is 0.053\n",
            "episode 154, the reward is -0.04\n",
            "episode 155, the reward is 0.15\n",
            "episode 156, the reward is 0.119\n",
            "episode 157, the reward is 0.165\n",
            "episode 158, the reward is 0.064\n",
            "episode 159, the reward is 0.234\n",
            "episode 160, the reward is 0.038\n",
            "episode 161, the reward is -0.016\n",
            "episode 162, the reward is 0.27\n",
            "episode 163, the reward is 0.275\n",
            "episode 164, the reward is -0.028\n",
            "episode 165, the reward is 0.121\n",
            "episode 166, the reward is 0.099\n",
            "episode 167, the reward is 0.126\n",
            "episode 168, the reward is 0.101\n",
            "episode 169, the reward is 0.005\n",
            "episode 170, the reward is 0.314\n",
            "episode 171, the reward is -0.063\n",
            "episode 172, the reward is 0.158\n",
            "episode 173, the reward is 0.195\n",
            "episode 174, the reward is 0.098\n",
            "episode 175, the reward is 0.093\n",
            "episode 176, the reward is 0.037\n",
            "episode 177, the reward is 0.141\n",
            "episode 178, the reward is 0.178\n",
            "episode 179, the reward is 0.149\n",
            "episode 180, the reward is 0.153\n",
            "episode 181, the reward is 0.348\n",
            "episode 182, the reward is 0.189\n",
            "episode 183, the reward is 0.205\n",
            "episode 184, the reward is -0.018\n",
            "episode 185, the reward is 0.174\n",
            "episode 186, the reward is 0.292\n",
            "episode 187, the reward is 0.153\n",
            "episode 188, the reward is 0.358\n",
            "episode 189, the reward is 0.171\n",
            "episode 190, the reward is -0.001\n",
            "episode 191, the reward is 0.37\n",
            "episode 192, the reward is 0.127\n",
            "episode 193, the reward is 0.225\n",
            "episode 194, the reward is 0.065\n",
            "episode 195, the reward is 0.1\n",
            "episode 196, the reward is -0.097\n",
            "episode 197, the reward is -0.09\n",
            "episode 198, the reward is 0.083\n",
            "episode 199, the reward is 0.158\n",
            "episode 200, the reward is -0.038\n",
            "episode 201, the reward is 0.085\n",
            "episode 202, the reward is 0.112\n",
            "episode 203, the reward is 0.266\n",
            "episode 204, the reward is -0.332\n",
            "episode 205, the reward is -0.078\n",
            "episode 206, the reward is 0.093\n",
            "episode 207, the reward is 0.033\n",
            "episode 208, the reward is 0.226\n",
            "episode 209, the reward is 0.178\n",
            "episode 210, the reward is 0.066\n",
            "episode 211, the reward is 0.018\n",
            "episode 212, the reward is 0.236\n",
            "episode 213, the reward is 0.342\n",
            "episode 214, the reward is 0.163\n",
            "episode 215, the reward is 0.085\n",
            "episode 216, the reward is 0.235\n",
            "episode 217, the reward is 0.13\n",
            "episode 218, the reward is 0.139\n",
            "episode 219, the reward is 0.001\n",
            "episode 220, the reward is 0.078\n",
            "episode 221, the reward is 0.079\n",
            "episode 222, the reward is 0.148\n",
            "episode 223, the reward is 0.046\n",
            "episode 224, the reward is 0.056\n",
            "episode 225, the reward is -0.008\n",
            "episode 226, the reward is -0.02\n",
            "episode 227, the reward is 0.07\n",
            "episode 228, the reward is -0.01\n",
            "episode 229, the reward is 0.06\n",
            "episode 230, the reward is 0.178\n",
            "episode 231, the reward is 0.23\n",
            "episode 232, the reward is 0.188\n",
            "episode 233, the reward is 0.12\n",
            "episode 234, the reward is 0.114\n",
            "episode 235, the reward is 0.126\n",
            "episode 236, the reward is -0.031\n",
            "episode 237, the reward is 0.179\n",
            "episode 238, the reward is 0.14\n",
            "episode 239, the reward is 0.046\n",
            "episode 240, the reward is 0.105\n",
            "episode 241, the reward is 0.098\n",
            "episode 242, the reward is 0.129\n",
            "episode 243, the reward is 0.216\n",
            "episode 244, the reward is 0.069\n",
            "episode 245, the reward is -0.018\n",
            "episode 246, the reward is -0.017\n",
            "episode 247, the reward is 0.202\n",
            "episode 248, the reward is 0.237\n",
            "episode 249, the reward is 0.084\n",
            "episode 250, the reward is 0.129\n",
            "episode 251, the reward is 0.226\n",
            "episode 252, the reward is 0.162\n",
            "episode 253, the reward is 0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tkkdkfHMtTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZqKDFiKGkUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make('MountainCar-v0'))\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "    action = agent.choose_action(observation) \n",
        "    observation, reward, done, info = env.step(action)   \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}